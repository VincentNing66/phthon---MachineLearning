{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Vincent\n#1345375\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/mnist_train.csv', dtype=int)\nX_train = train.drop('label', axis=1)\ny_train = train['label']\ntest = pd.read_csv('../input/mnist_test.csv', dtype=int)\nX_test = test.drop('label', axis=1)\ny_test = test['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth=[10,20,30,40,50,60]\nbest_oob_score = 0\ndepthpoint = 0\nbest_ac = 0\nbest_odf = []\nbest_proba_1 = []\nfor d in range(len(max_depth)):\n    model = ExtraTreesClassifier(bootstrap = True, oob_score=True,max_depth = max_depth[d],n_estimators=300, n_jobs=-1, random_state=1345375)\n    model.fit(X_train,y_train)\n    score = model.oob_score_\n    preds = model.predict(X_test)\n    ac = accuracy_score(y_test,preds)\n    odf = model.oob_decision_function_\n    proba = model.predict_proba(X_test) \n    print(str(score)+\" \"+str(max_depth[d])+\" \"+str(ac))\n    if score > best_oob_score:\n        best_oob_score = score\n        depthpoint = max_depth[d]\n        best_ac = ac\n        best_odf = odf\n        best_proba_1 = proba\nprint(\"Best score \"+str(best_oob_score)+\" Depth \"+str(depthpoint)+\" Accurancy_score \" + str(best_ac))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth=[10,20,30,40,50,60]\nbest_oob_score_2 = 0\ndepthpoint_2 = 0\nbest_ac_2 = 0\nbest_odf_2 = []\nbest_proba_2 = []\ndef single_extra_tree(depth=None, seed=1345375):\n    return ExtraTreesClassifier(max_depth=depth, n_estimators=1, bootstrap=False, random_state=seed)\nfor d in range(len(max_depth)):\n    single_extra_tree\n    boost = AdaBoostClassifier(base_estimator=single_extra_tree(depth=max_depth[d]), n_estimators=10, random_state=1345375)\n    bag = BaggingClassifier(base_estimator=boost, n_estimators=30, bootstrap=True, oob_score=True, n_jobs=-1, random_state=1345375)\n    bag.fit(X_train,y_train)\n    score = bag.oob_score_\n    preds = bag.predict(X_test)\n    ac = accuracy_score(y_test,preds)\n    odf = bag.oob_decision_function_\n    proba = bag.predict_proba(X_test)\n    print(str(score)+\" \"+str(max_depth[d])+\" \"+str(ac))\n    if score > best_oob_score_2:\n        best_oob_score_2 = score\n        depthpoint_2 = max_depth[d]\n        best_ac_2 = ac\n        best_odf_2 = odf\n        best_proba_2 = proba\nprint(\"Best score \"+str(best_oob_score_2)+\" Depth \"+str(depthpoint_2)+\" Accurancy_score \" + str(best_ac_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_components=[20, 40, 60]\nbest_oob_score_3 = 0\nnumberpoint_3 = 0\nbest_ac_3 = 0\nbest_odf_3 = []\nbest_proba_3 = []\nfor d in range(len(num_components)):\n    pca = PCA(n_components=num_components[d], svd_solver='randomized', random_state=1345375)\n    pip = Pipeline( [ ('pca', pca), ('ada_xt', AdaBoostClassifier(base_estimator=single_extra_tree(depth=depthpoint_2), n_estimators=10, random_state=1345375)) ])\n    bag = BaggingClassifier(base_estimator=pip, n_estimators=30, bootstrap=True, oob_score=True, n_jobs=-1, random_state=1345375)\n    bag.fit(X_train,y_train)\n    score = bag.oob_score_\n    preds = bag.predict(X_test)\n    ac = accuracy_score(y_test,preds)\n    odf = bag.oob_decision_function_\n    proba = bag.predict_proba(X_test)\n    print(str(score)+\" \"+str(num_components[d])+\" \"+str(ac))\n    if score > best_oob_score_3:\n        best_oob_score_3 = score\n        numberpoint_3 = num_components[d]\n        best_ac_3 = ac\n        best_odf_3 = odf\n        best_proba_3 = proba\nprint(\"Best score \"+str(best_oob_score_3)+\" number \"+str(numberpoint_3)+\" Accurancy_score \" + str(best_ac_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums1 = np.sum( [best_odf, best_odf_2, best_odf_3], axis=0)\naccuracy_score(y_train,  np.argmax(sums1, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums2 = np.sum( [best_odf, best_odf_2], axis=0)\naccuracy_score(y_train,  np.argmax(sums2, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums3 = np.sum( [best_odf_2, best_odf_3], axis=0)\naccuracy_score(y_train,  np.argmax(sums3, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums4 = np.sum( [best_odf, best_odf_3], axis=0)\naccuracy_score(y_train,  np.argmax(sums4, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums5 = np.sum( [best_odf], axis=0)\naccuracy_score(y_train,  np.argmax(sums5, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums6 = np.sum( [best_odf_2], axis=0)\naccuracy_score(y_train,  np.argmax(sums6, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums7 = np.sum( [best_odf_3], axis=0)\naccuracy_score(y_train,  np.argmax(sums7, axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sums5 has the highest oob accuracy estimate<br>\nwhich is only the ExtraTreesClassifier it self"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_meta = np.concatenate( [best_odf, best_odf_2, best_odf_3], axis=1)\nX_test_meta = np.concatenate( [best_proba_1, best_proba_2, best_proba_3], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log = LogisticRegression(random_state=1345375, C=50).fit(X_train_meta,y_train)\nlog_pred=log.predict(X_test_meta)\nlog_ac = accuracy_score(y_test,log_pred)\nprint(log_ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is 0.9728<br>\nYes, it is better than simple voting, due to the reason that it has higher accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=0\ndef misclassfied(n):\n    item=n.values.reshape(28,28)\n    plt.imshow(item, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n    plt.axis(\"off\")\n    plt.show()\n    \nfor i in range(len(y_test)):\n    if x == 11:\n        break\n    elif y_test[i]!=log_pred[i] and y_test[i] == x:\n        print(y_test[i], \"got wrong to : \", log_pred[i])\n        misclassfied(X_test[i:i+1])\n        x=x+1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}