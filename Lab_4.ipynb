{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#ID:1345375\n#Vincent Ning\n\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/fashion-mnist_train.csv', dtype=int)\nX_train = train.drop('label', axis=1)\ny_train = train['label']\nX_train[0:1].values.reshape(28,28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/fashion-mnist_test.csv', dtype=int)\nX_test = test.drop('label', axis=1)\ny_test = test['label']\nimg1 = X_train[0:1].values.reshape(28,28)\nplt.imshow(img1, interpolation=\"nearest\")\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img2 = X_train[1:2].values.reshape(28,28)\nplt.imshow(img2, interpolation=\"nearest\")\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img3 = X_train[2:3].values.reshape(28,28)\nplt.imshow(img3, interpolation=\"nearest\")\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gclf=GaussianNB()\ngclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = gclf.predict(X_test)\nac = accuracy_score(y_test,preds)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grid_search(clf, X_train, y_train, n_trees=30):\n    max_features =[1,4,16,64,'auto']\n    max_depth=[1,4,16,64,None]\n    best_oob_score = 0\n    featurepoint = 0\n    depthpoint = 0\n    for f in range(len(max_features)):\n        for d in range(len(max_depth)):\n            model = clf(bootstrap = True, oob_score=True,max_depth = max_depth[d],max_features=max_features[f],n_estimators=n_trees, n_jobs=-1, random_state=1345375)\n            model.fit(X_train,y_train)\n            score = model.oob_score_\n            print(str(score)+\" \"+str(max_features[f])+\" \"+str(max_depth[d]))\n            if score > best_oob_score:\n                best_oob_score = score\n                featurepoint= max_features[f]\n                depthpoint = max_depth[d]\n    print(\"best score \"+str(best_oob_score)+\" Features \"+str(featurepoint)+\" Depth \"+str(depthpoint))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier\ngrid_search(rf,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best = RandomForestClassifier(bootstrap = True, oob_score=True ,max_depth=64, max_features=64,n_estimators=100, n_jobs=-1, random_state=1345375)\nrf_best.fit(X_train,y_train)\ny_pred_rf_best = rf_best.predict(X_test)\nrf_best_accuracy = accuracy_score(y_test,y_pred_rf_best)\nprint(\"accuracy = \" + str(rf_best_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cm = confusion_matrix(y_test, y_pred_rf_best)\nprint(rf_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cm = pd.DataFrame(rf_cm, range(10), range(10))\nplt.figure(figsize=(14,7))\n#sns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 12})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_best.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.matshow(rf_best.feature_importances_.reshape(28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et = ExtraTreesClassifier\ngrid_search(et,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et_best = ExtraTreesClassifier(bootstrap = True, oob_score=True ,max_depth=64, max_features=64,n_estimators=100, n_jobs=-1, random_state=1345375)\net_best.fit(X_train,y_train)\ny_pred_et_best = et_best.predict(X_test)\net_best_accuracy = accuracy_score(y_test,y_pred_et_best)\nprint(\"accuracy = \" + str(et_best_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et_cm = confusion_matrix(y_test, y_pred_et_best)\nprint(et_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_cm = pd.DataFrame(et_cm, range(10), range(10))\nplt.figure(figsize=(14,7))\n#sns.set(font_scale=1.4)#for label size\nsns.heatmap(df1_cm, annot=True,annot_kws={\"size\": 12})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(et_best.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.matshow(et_best.feature_importances_.reshape(28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_test = RandomForestClassifier()\nrf_test.fit(X_train,y_train)\npreds_rf_test = rf_test.predict(X_test)\nac = accuracy_score(y_test,preds_rf_test)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et_test = ExtraTreesClassifier()\net_test.fit(X_train,y_train)\npreds_et_test = et_test.predict(X_test)\nac2 = accuracy_score(y_test,preds_et_test)\nprint(ac2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Regarding OOB scores, is the RandomForestClassifier better than the ExtraTreesClassifier?\n\nBased on the output: yes, it is.\n\n\n- Is the OOB score for the RandomForestClassifier lower than its test-set accuracy?\n\nNo, OOB score is higher\n\n\n- Is the OOB score for the ExtraTreesClassifier lower than its test-set accuracy?\n\nNo, OOB score is higher\n\n\n- If you use the OOB scores to choose the best classifier, is that consistent with the test-set accuracies?\n\nNo\n\n\n- Do the feature_importances matrix plots make sense (explain)?\n\nyes, the bright color it is, more importance it is, in those two  heat maps, the most important data is at center bottom, and for coners, they are never been used\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}